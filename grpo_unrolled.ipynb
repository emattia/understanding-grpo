{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torchtune import training, generation, rlhf\n",
    "from torchtune.modules import local_kv_cache\n",
    "from torchtune.dev.grpo.generation import generate\n",
    "from torchtune.dev.grpo.types import GRPOTrajectory\n",
    "from omegaconf import OmegaConf\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¨ [Clone torchtune repo](https://github.com/pytorch/torchtune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtune_path = '/home/eddie/dev/torchtune' # TODO: update to where you checked out the torchtune repo.\n",
    "grpo_config = 'dev/3B_full_grpo.yaml'\n",
    "sft_grpo_config = 'dev/3B_sft_for_grpo.yaml'\n",
    "grpo_recipe_implementation_parent = os.path.join(torchtune_path, 'recipes', 'dev/grpo_full_finetune_distributed.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to import torchtune recipe in REPL env like a notebook.\n",
    "import importlib.util\n",
    "module_name = \"grpo_recipe\"\n",
    "spec = importlib.util.spec_from_file_location(module_name, grpo_recipe_implementation_parent)\n",
    "grpo_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(grpo_module)\n",
    "FullGRPOFinetuneRecipeDistributed = grpo_module.FullGRPOFinetuneRecipeDistributed\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '8000'\n",
    "os.environ['WANDB_API_KEY'] = input('Paste WANDB key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting manual seed to local seed 1465723292. Local seed is seed + rank = 1465723292 + 0\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(os.path.join(torchtune_path, 'recipes/configs', grpo_config))\n",
    "recipe = FullGRPOFinetuneRecipeDistributed(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meddiem\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eddie/dev/understanding-grpo/wandb/run-20250303_181918-f6np6kq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eddiem/torchtune-test/runs/f6np6kq3' target=\"_blank\">worthy-snowball-5</a></strong> to <a href='https://wandb.ai/eddiem/torchtune-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eddiem/torchtune-test' target=\"_blank\">https://wandb.ai/eddiem/torchtune-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eddiem/torchtune-test/runs/f6np6kq3' target=\"_blank\">https://wandb.ai/eddiem/torchtune-test/runs/f6np6kq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "Instantiating model and loading checkpoint took 2.16 secs\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory active: 12.11 GiB\n",
      "\tGPU peak memory alloc: 12.11 GiB\n",
      "\tGPU peak memory reserved: 12.16 GiB\n",
      "Optimizer is initialized.\n",
      "Loss is initialized.\n",
      "Dataset and Sampler are initialized.\n",
      "Learning rate scheduler is initialized.\n",
      "`profile_memory` requires `with_stack` and `record_shapes`, these will be enabled since `profile_memory` is True\n",
      " Profiler config after instantiation: {'enabled': True, 'output_dir': '/tmp/checkpoints/grpo_llama3b/profiling_outputs', 'cpu': True, 'cuda': True, 'xpu': True, 'profile_memory': True, 'with_stack': True, 'record_shapes': True, 'with_flops': False, 'wait_steps': 5, 'warmup_steps': 3, 'active_steps': 2, 'num_cycles': 1}\n"
     ]
    }
   ],
   "source": [
    "recipe.setup(cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=recipe # trick this unrolled notebook into acting like the actual class\n",
    "training.cleanup_before_training()\n",
    "self._optimizer.zero_grad()\n",
    "grad_norm = None\n",
    "training_completed = False\n",
    "self._profiler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch=0\n",
    "self._sampler.set_epoch(curr_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does a single batch of data look like?\n",
    "\n",
    "Note that the batch size is set in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A conversation between User and Assistant. The user asks a question, and the '\n",
      " 'Assistant solves it. The assistant first thinks about the reasoning process '\n",
      " 'in the mind and then provides the user with the answer. The reasoning '\n",
      " 'process and answer are enclosed within <think></think> and <answer></answer> '\n",
      " 'tags, respectively, i.e., <think>reasoning process here</think> '\n",
      " '<answer>answer here</answer>. User: Weng earns $12 an hour for babysitting. '\n",
      " 'Yesterday, she just did 50 minutes of babysitting. How much did she earn?. '\n",
      " 'Assistant: <think>\\n'\n",
      " '----------\\n'\n",
      " 'GROUND_TRUTH_ANSWER: 10')\n"
     ]
    }
   ],
   "source": [
    "batch = next(self._dataloader._get_iterator())\n",
    "tokens = batch[\"tokens\"]\n",
    "answers = batch[\"answers\"]\n",
    "tokens = tokens.to(self._device) # [batch_size x num_tokens]\n",
    "tokens_ls = tokens.tolist()\n",
    "out = []\n",
    "for i in range(tokens.shape[0]):\n",
    "    prompt = self._tokenizer.decode(tokens_ls[i])\n",
    "    answer = answers[i]\n",
    "    out.append(prompt+'\\n' + '-'*10+'\\n' + 'GROUND_TRUTH_ANSWER: ' + answer)\n",
    "sep = \"-\"*80\n",
    "pprint(sep.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, context_length = tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a trajectory\n",
    "\n",
    "The main work within each iteration is to generate a \"trajectory\". \n",
    "These two functions drive the current codebase.\n",
    "\n",
    "```python\n",
    "def generate_trajectory_batched(\n",
    "        self, input_ids: torch.Tensor, answers: List[str]\n",
    "    ) -> GRPOTrajectory:\n",
    "        \"\"\"\n",
    "        Generates a ``self.batch_size`` batch of trajectories using `self._forward_batch_size` batch sizes.\n",
    "        See ``generate_trajectory`` for more details.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): tensor of input token IDs with shape [b, seq_length]\n",
    "            answers: (List[str]): list of answers corresponding to the input_ids\n",
    "\n",
    "        Returns:\n",
    "            Trajectory: An instance of :class:`~torchtune.rlhf.Trajectory`, comprising\n",
    "                the current trajectory.\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "The work of this function happens in \n",
    "```python\n",
    "trajectories.append(\n",
    "    self.generate_trajectory(batch_input_ids, batch_answers)\n",
    ")\n",
    "```\n",
    "\n",
    "This function has the following signature:\n",
    "```python\n",
    "def generate_trajectory(\n",
    "    self, input_ids: torch.Tensor, answers: List[str]\n",
    ") -> GRPOTrajectory:\n",
    "    \"\"\"\n",
    "    Generates a trajectory given the current policy model, the reference policy model, the reward function,\n",
    "    and batch of inputs. This is done over the following steps:\n",
    "\n",
    "    1: Generate responses, and logits corresponding to the responses using the current policy,\n",
    "        generating (query, response) pairs.\n",
    "    2. Estimate logprobs of the generated responses using the current policy.\n",
    "    3. Compute rewards and successes for the generated responses.\n",
    "    4. Estimate advantages using GRPO.\n",
    "    5. Replace any tokens in the response after the first stop token (usually EOS token) with padding,\n",
    "        producing truncated responses.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): tensor of input token IDs with shape [b, seq_length]\n",
    "        answers (List[str]): list of answers corresponding to the input_ids\n",
    "\n",
    "    Returns:\n",
    "        Trajectory: An instance of :class:`~torchtune.rlhf.GRPOTrajectory` comprising\n",
    "            the current trajectory.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "This function contains the logic we want to step through to understand what drives learning in GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory = self.generate_trajectory_batched(tokens, answers)\n",
    "batch_input_ids = tokens[\n",
    "    batch_start : batch_start + self._forward_batch_size\n",
    "]\n",
    "batch_answers = answers[\n",
    "    batch_start : batch_start + self._forward_batch_size\n",
    "]\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of generate_trajectory\n",
    "\n",
    "batch_size, context_length = batch_input_ids.shape\n",
    "grpo_size = self.grpo_samples\n",
    "\n",
    "batch_input_ids = batch_input_ids[:, None, :].expand(-1, grpo_size, -1)  # [batch_size, grpo_size, L]\n",
    "batch_input_ids = batch_input_ids.reshape(batch_size * grpo_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innermost generation\n",
    "\n",
    "At this point, we pass the tokenized batch of prompts into the core `generate` function.\n",
    "For GRPO, there is a custom version implemented, here is the signature:\n",
    "\n",
    "```python\n",
    "# NOTE: This is almost the same as torchtune.generation.generate, with a few changes necessary for GRPO.\n",
    "# Namely:\n",
    "#   1. The `return_logits` argument - we can optionally omit keeping track of logits during generation, which\n",
    "#        drastically improves generation speed.\n",
    "#   2. Stop token-based breaking now communicates across multiple devices in a distributed setting.\n",
    "# TODO: Figure out the right abstractions to be used in the main repository, and remove this function.\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: TransformerDecoder,\n",
    "    prompt: torch.Tensor,\n",
    "    *,\n",
    "    max_generated_tokens: int,\n",
    "    pad_id: int = 0,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    stop_tokens: Optional[List[int]] = None,\n",
    "    rng: Optional[torch.Generator] = None,\n",
    "    custom_generate_next_token: Optional[Callable] = None,\n",
    "    return_logits: bool = True,\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb85d00a241490ab542f7861b86587f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with local_kv_cache(\n",
    "    model=self._model,\n",
    "    batch_size=batch_size * grpo_size,\n",
    "    device=self._device,\n",
    "    dtype=self._dtype,\n",
    "    decoder_max_seq_len=context_length + self._max_generated_tokens,\n",
    "):\n",
    "    query_responses, _ = generate(  # [B x G, L], [B x G, L, V]\n",
    "        model=self._model,\n",
    "        prompt=batch_input_ids,\n",
    "        max_generated_tokens=self._max_generated_tokens,\n",
    "        temperature=self._temperature,\n",
    "        top_k=self._top_k,\n",
    "        pad_id=self._tokenizer.pad_id,\n",
    "        rng=self._rng,\n",
    "        stop_tokens=self._tokenizer.stop_tokens,\n",
    "        return_logits=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert query_responses.shape[0] == grpo_size, 'The generate function is not making the correct number of GRPO samples.'\n",
    "assert query_responses.shape[1] <= self._max_generated_tokens, 'The generate function is not respecting max_generated_tokens.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.barrier()\n",
    "training._distributed.recursive_reshard(self._model)\n",
    "torch.cuda.empty_cache()\n",
    "responses = query_responses[:, context_length:].clone()\n",
    "query_response_padding_masks = query_responses != self._tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solid color --> no padding tokens\n",
    "plt.imshow(query_response_padding_masks.cpu());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create attention masks and position IDs for any padding tokens in inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to mask the scores after the query-key multiplication and before the softmax. \n",
    "# This parameter is required during inference if caches have been setup.\n",
    "# A value of True in row ``i`` and column ``j`` means token ``i`` attends to token ``j``.\n",
    "masks = generation.get_causal_mask_from_padding_mask(\n",
    "    query_response_padding_masks\n",
    ")\n",
    "\n",
    "# Contains the position ids of each token. \n",
    "# This parameter is required during inference if caches have been setup.\n",
    "# During training, this is used to indicate the positions of each token relative to its sample.\n",
    "# During inference, this indicates the position of the current token.\n",
    "position_ids = generation.get_position_ids_from_padding_mask(\n",
    "    query_response_padding_masks\n",
    ")\n",
    "del query_response_padding_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEXCAYAAACUBEAgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIZhJREFUeJzt3X1Uk+fhPvArqEmrkFDUEDiCtbUVqWj9UotZO79dpaBiqy3dmdYqdfx0uuA5ivVnszm6buvidGd9my+nexG3lrrZU6yyomPIS60RkUpFqrR6VGgxYGUkQDW85P7+4XjWVFQCSZ68XJ9znnOaPHeS+9Z6nVy5A49CCCFARORlIXJPgIiCE8OHiGTB8CEiWTB8iEgWDB8ikgXDh4hkwfAhIlkwfIhIFgwfIpIFw4eIZCFb+GzZsgV33nknbrvtNiQlJeHo0aNyTYWIZCBL+Pztb39DdnY2XnzxRXz88ceYMmUKUlNT0dzcLMd0iEgGCjl+sDQpKQnTpk3D73//ewCAw+FATEwMVq1ahRdeeOGWj3c4HGhsbERYWBgUCoWnp0tE/SSEQFtbG6KjoxEScvP3NkO9NCdJZ2cnqqqqYDQapftCQkKQnJwMs9nc52Psdjvsdrt0+8svv0R8fLzH50pEA9PQ0IAxY8bcdIzXw+err75CT08PIiMjne6PjIzE6dOn+3yMyWTCSy+9dN39e9+YhD/mjIXt315fBhH1oRtdOIQPEBYWdsuxfvGv1mg0Ijs7W7pts9kQExOD76Z+jShtI17+0Vi0W/1iKUSB7T8f4vTn4xCvf+A8atQoDBkyBE1NTU73NzU1QafT9fkYlUoFtVrtdPSa+t02/HT7Bagjuj06byJyL6+Hj1KpRGJiIoqLi6X7HA4HiouLodfrXX4+hQL4n/9tw/9/ox4aBhCR35Blqz07Oxt/+MMfsHPnTpw6dQorV65ER0cHli5dOuDnnPY9G17YegGhGgYQkT+Q5YOSH/zgB7h06RJycnJgsVhw//33Y//+/dd9CO2q3gpmMoyFrYWfARH5Mlm+5zNYNpsNGo0G//7sLqjDrn/zVlmixuZVsbAygIi8qlt0oRTvw2q1On0225eA/NkuVjAi3xeQ4QNwF4zI1wVs+HAXjMi3BWz49GIFI/JNAR8+ACsYkS8KivBhBSPyPUERPr1YwYh8R1CFD8AKRuQrgi58WMGIfEPQhU8vVjAieQVt+ACsYERyCurwYQUjkk9Qh08vVjAi72P4/AcrGJF3MXz+gxWMyLsYPt/CCkbkHQyfPrCCEXkew6cPrGBEnsfwuQlWMCLPcXv4/PznP4dCoXA64uLipPNXr16FwWDAyJEjERoaivT09Ouu4eVLWMGIPMMj73zuu+8+XLx4UToOHToknVuzZg327duH3bt3o6ysDI2NjXjqqac8MQ23YAUj8gyPXN5h6NChfV591Gq14k9/+hPy8vLw6KOPAgB27NiBiRMn4siRI5g+fbonpuMWvRWMl2Ymcg+PvPP5/PPPER0djbvuuguLFi1CfX09AKCqqgpdXV1ITk6WxsbFxSE2NhZms/mGz2e322Gz2ZwOObCCEbmP28MnKSkJubm52L9/P7Zt24Zz587hu9/9Ltra2mCxWKBUKhEeHu70mMjISFgslhs+p8lkgkajkY6YmBh3T7tfWMGI3Mft4TN79mx8//vfx+TJk5GamooPPvgAra2t+Pvf/z7g5zQajbBardLR0NDgxhm7jrtgRIPn8a328PBw3HvvvThz5gx0Oh06OzvR2trqNKapqanPz4h6qVQqqNVqp0NurGBEg+Px8Glvb8fZs2cRFRWFxMREDBs2DMXFxdL5uro61NfXQ6/Xe3oqbsUKRjQ4bg+f559/HmVlZTh//jwOHz6MJ598EkOGDMHChQuh0WiQmZmJ7OxslJSUoKqqCkuXLoVer/fpna6bYQUjGhi37xl/8cUXWLhwIS5fvozRo0fj4YcfxpEjRzB69GgAwCuvvIKQkBCkp6fDbrcjNTUVW7dudfc0vKq3gpkMY2Fr4TY8UX8ohBBC7km4ymazQaPR4N+f3QV1mO/8hEhliRqbV8XCygCiINUtulCK92G1Wm/52azv/MsNAKxgRP3H8HEz7oIR9Q/Dx824C0bUPwwfD2EFI7o5ho8HsYIR3RjDx4NYwYhujOHjBaxgRNdj+HgJKxiRM4aPl7CCETlj+HgZKxjRNQwfGbCCETF8ZMEKRsTwkRUrGAUzho/MWMEoWDF8ZMYKRsGK4eMjWMEo2DB8fAgrGAUTho8PYQWjYMLw8UGsYBQMXA6f8vJyPP7444iOjoZCocCePXuczgshkJOTg6ioKNx+++1ITk7G559/7jSmpaUFixYtglqtRnh4ODIzM9He3j6ohQQaVjAKdC6HT0dHB6ZMmYItW7b0eX7Tpk14/fXXsX37dlRUVGDEiBFITU3F1atXpTGLFi1CbW0tioqKUFBQgPLycixfvnzgqwhArGAU6AZ19QqFQoH8/HzMnz8fwLV3PdHR0Vi7di2ef/55AIDVakVkZCRyc3OxYMECnDp1CvHx8aisrMQDDzwAANi/fz/mzJmDL774AtHR0bd8XV+9eoWnfFwehpd/NBbtVl4Vg3ybbFevOHfuHCwWC5KTk6X7NBoNkpKSYDabAQBmsxnh4eFS8ABAcnIyQkJCUFFR0efz2u122Gw2pyOYsIJRIHJr+FgsFgBAZGSk0/2RkZHSOYvFAq1W63R+6NChiIiIkMZ8m8lkgkajkY6YmBh3TtvnsYJRIPKLzmI0GmG1WqWjoaFB7inJgrtgFEjcGj46nQ4A0NTU5HR/U1OTdE6n06G5udnpfHd3N1paWqQx36ZSqaBWq52OYMUKRoHCreEzbtw46HQ6FBcXS/fZbDZUVFRAr9cDAPR6PVpbW1FVVSWNOXjwIBwOB5KSktw5nYDECkaBwuXwaW9vR3V1NaqrqwFc+5C5uroa9fX1UCgUWL16NX71q19h7969qKmpwZIlSxAdHS3tiE2cOBGzZs3CsmXLcPToUXz00UfIysrCggUL+rXTRdewgpG/c3mrvbS0FN/73veuuz8jIwO5ubkQQuDFF1/Em2++idbWVjz88MPYunUr7r33XmlsS0sLsrKysG/fPoSEhCA9PR2vv/46QkND+zWHYNtqvxEhgOPlYTAZxsLWwm14kp8rW+2D+p6PXBg+zipL1Ni8KhZWBhDJTLbv+ZA8WMHIHzF8AgR3wcjfMHwCBHfByN8wfAIMKxj5C4ZPAGIFI3/A8AlArGDkDxg+AYwVjHwZwyfAsYKRr2L4BDhWMPJVDJ8gwQpGvobhE0RYwciXMHyCCCsY+RKGTxBiBSNfwPAJUqxgJDeGT5BiBSO5MXyCHCsYyYXhQ6xgJAuGD7GCkSwYPiRhBSNvcjl8ysvL8fjjjyM6OhoKhQJ79uxxOv/cc89BoVA4HbNmzXIa09LSgkWLFkGtViM8PByZmZlob28f1ELIPVjByFtcDp+Ojg5MmTIFW7ZsueGYWbNm4eLFi9LxzjvvOJ1ftGgRamtrUVRUhIKCApSXl2P58uWuz57cjhWMvMXlyx3Mnj0bs2fPvukYlUp1w6uPnjp1Cvv370dlZSUeeOABAMAbb7yBOXPm4Le//S2v3eUjeivYyz8ai3Yrr4pB7ueRz3xKS0uh1WoxYcIErFy5EpcvX5bOmc1mhIeHS8EDAMnJyQgJCUFFRUWfz2e322Gz2ZwO8jxWMPIkt4fPrFmz8Je//AXFxcX4zW9+g7KyMsyePRs9PT0AAIvFAq1W6/SYoUOHIiIiAhaLpc/nNJlM0Gg00hETE+PuaVMfWMHIk9wePgsWLMATTzyBhIQEzJ8/HwUFBaisrERpaemAn9NoNMJqtUpHQ0OD+yZMt8RdMPIEj2+133XXXRg1ahTOnDkDANDpdGhubnYa093djZaWlht+TqRSqaBWq50O8i5WMHI3j4fPF198gcuXLyMqKgoAoNfr0draiqqqKmnMwYMH4XA4kJSU5Onp0ACxgpG7uRw+7e3tqK6uRnV1NQDg3LlzqK6uRn19Pdrb27Fu3TocOXIE58+fR3FxMebNm4fx48cjNTUVADBx4kTMmjULy5Ytw9GjR/HRRx8hKysLCxYs4E6XH2AFI3dxOXyOHTuGqVOnYurUqQCA7OxsTJ06FTk5ORgyZAhOnDiBJ554Avfeey8yMzORmJiIDz/8ECqVSnqOt99+G3FxcZg5cybmzJmDhx9+GG+++ab7VkUexQpG7qAQQgi5J+Eqm80GjUaDf392F9Rh/AkRuVSWqLF5VSysLfweEF3TLbpQivdhtVpv+dks/+XSgLGC0WAwfGhQWMFooBg+NCjcBaOBYviQW7CCkasYPuQ2rGDkCoYPuQ0rGLmC4UNuxwpG/cHwIY9gBaNbYfiQR7CC0a0wfMijWMHoRhg+5HGsYNQXhg95HCsY9YXhQ17DCkbfxPAhr2IFo14MH/IqVjDqxfAhWbCCEcOHZMMKFtwYPiQbVrDgxvAh2bGCBSeXwsdkMmHatGkICwuDVqvF/PnzUVdX5zTm6tWrMBgMGDlyJEJDQ5Geno6mpianMfX19UhLS8Pw4cOh1Wqxbt06dHfzf7xgxgoWfFwKn7KyMhgMBhw5cgRFRUXo6upCSkoKOjo6pDFr1qzBvn37sHv3bpSVlaGxsRFPPfWUdL6npwdpaWno7OzE4cOHsXPnTuTm5iInJ8d9qyK/wwoWfAZ19YpLly5Bq9WirKwMM2bMgNVqxejRo5GXl4enn34aAHD69GlMnDgRZrMZ06dPR2FhIebOnYvGxkZERkYCALZv347169fj0qVLUCqVt3xdXr0isH1cHoaXfzQW7VZeFcPfeO3qFVarFQAQEREBAKiqqkJXVxeSk5OlMXFxcYiNjYXZbAYAmM1mJCQkSMEDAKmpqbDZbKitre3zdex2O2w2m9NBgYsVLDgMOHwcDgdWr16Nhx56CJMmTQIAWCwWKJVKhIeHO42NjIyExWKRxnwzeHrP957ri8lkgkajkY6YmJiBTpv8ACtYcBhw+BgMBpw8eRK7du1y53z6ZDQaYbVapaOhocHjr0ny4y5YYBtQ+GRlZaGgoAAlJSUYM2aMdL9Op0NnZydaW1udxjc1NUGn00ljvr371Xu7d8y3qVQqqNVqp4OCAytY4HIpfIQQyMrKQn5+Pg4ePIhx48Y5nU9MTMSwYcNQXFws3VdXV4f6+nro9XoAgF6vR01NDZqbm6UxRUVFUKvViI+PH8xaKACxggUul8LHYDDgrbfeQl5eHsLCwmCxWGCxWHDlyhUAgEajQWZmJrKzs1FSUoKqqiosXboUer0e06dPBwCkpKQgPj4eixcvxieffIIDBw5gw4YNMBgMUKlU7l8hBQRWsMDj0la7QqHo8/4dO3bgueeeA3DtS4Zr167FO++8A7vdjtTUVGzdutWpUl24cAErV65EaWkpRowYgYyMDGzcuBFDh/Zva5Vb7cFJCOB4eRhMhrGwtXAb3he5stU+qO/5yIXhE9wqS9TYvCoWVgaQz/Ha93yI5MAKFhgYPuSXuAvm/xg+5Je4C+b/GD7k11jB/BfDh/weK5h/YviQ32MF808MHwoYrGD+heFDAYUVzH8wfCigsIL5D4YPBSRWMN/H8KGAxQrm2xg+FLBYwXwbw4cCHiuYb2L4UFBgBfM9DB8KCqxgvofhQ0GFFcx3MHwo6LCC+QaGDwUdVjDfwPChoMUKJi+XwsdkMmHatGkICwuDVqvF/PnzUVdX5zTmkUcegUKhcDpWrFjhNKa+vh5paWkYPnw4tFot1q1bh+5u/g9A3scKJh+XwqesrAwGgwFHjhxBUVERurq6kJKSgo6ODqdxy5Ytw8WLF6Vj06ZN0rmenh6kpaWhs7MThw8fxs6dO5Gbm4ucnBz3rIjIBaxg8hnU1SsuXboErVaLsrIyzJgxA8C1dz73338/Xn311T4fU1hYiLlz56KxsVG6Rvv27duxfv16XLp0CUql8pavy6tXkCd8XB6Gl380Fu1WXhVjoLx29Qqr1QoAiIiIcLr/7bffxqhRozBp0iQYjUZ8/fXX0jmz2YyEhAQpeAAgNTUVNpsNtbW1fb6O3W6HzWZzOojcjRXMuwYcPg6HA6tXr8ZDDz2ESZMmSfc/88wzeOutt1BSUgKj0Yi//vWvePbZZ6XzFovFKXgASLctFkufr2UymaDRaKQjJiZmoNMmuiFWMO8a8PtLg8GAkydP4tChQ073L1++XPrvhIQEREVFYebMmTh79izuvvvuAb2W0WhEdna2dNtmszGAyGN6d8FYwTxrQO98srKyUFBQgJKSEowZM+amY5OSkgAAZ86cAQDodDo0NTU5jem9/c1LKn+TSqWCWq12Oog8iRXM81wKHyEEsrKykJ+fj4MHD2LcuHG3fEx1dTUAICoqCgCg1+tRU1OD5uZmaUxRURHUajXi4+NdmQ6Rx7CCeZ5L4WMwGPDWW28hLy8PYWFhsFgssFgsuHLlCgDg7Nmz+OUvf4mqqiqcP38ee/fuxZIlSzBjxgxMnjwZAJCSkoL4+HgsXrwYn3zyCQ4cOIANGzbAYDBApVK5f4VEg8AvInqOS1vtCoWiz/t37NiB5557Dg0NDXj22Wdx8uRJdHR0ICYmBk8++SQ2bNjgVJUuXLiAlStXorS0FCNGjEBGRgY2btyIoUP716+51U7eJARwvDwMJsNY2Fr4GdDNuLLVPqjv+ciF4UNyqCxRY/OqWFgZQDfkte/5EAUTVjD3YvgQuYC7YO7D8CFyAXfB3IfhQzQArGCDx/AhGiBWsMFh+BANECvY4DB8iAaJFWxgGD5EbsAK5jqGD5EbsIK5juFD5EasYP3H8CFyM1aw/mH4ELkZK1j/MHyIPIQV7OYYPkQexAp2YwwfIg9iBbsxhg+RF7CCXY/hQ+QlrGDOGD5EXsIK5ozhQ+RlrGDXuBQ+27Ztw+TJk6VrZ+n1ehQWFkrnr169CoPBgJEjRyI0NBTp6enXXaOrvr4eaWlpGD58OLRaLdatW4fu7uD+S6DgwwrmYviMGTMGGzduRFVVFY4dO4ZHH30U8+bNk66xvmbNGuzbtw+7d+9GWVkZGhsb8dRTT0mP7+npQVpaGjo7O3H48GHs3LkTubm5yMnJce+qiHwcK5gbrl4RERGBzZs34+mnn8bo0aORl5eHp59+GgBw+vRpTJw4EWazGdOnT0dhYSHmzp2LxsZG6frs27dvx/r163Hp0iUolcp+vSavXkGB5OPysIC5NLNXrl7R09ODXbt2oaOjA3q9HlVVVejq6kJycrI0Ji4uDrGxsTCbzQAAs9mMhIQEKXgAIDU1FTabTXr31Be73Q6bzeZ0EAWKYK1gLodPTU0NQkNDoVKpsGLFCuTn5yM+Ph4WiwVKpRLh4eFO4yMjI2GxWAAAFovFKXh6z/eeuxGTyQSNRiMdMTExrk6byGcFawVzOXwmTJiA6upqVFRUYOXKlcjIyMCnn37qiblJjEYjrFardDQ0NHj09YjkEGy7YC6Hj1KpxPjx45GYmAiTyYQpU6bgtddeg06nQ2dnJ1pbW53GNzU1QafTAQB0Ot11u1+9t3vH9EWlUkk7bL0HUSAKpgo26E9rHQ4H7HY7EhMTMWzYMBQXF0vn6urqUF9fD71eDwDQ6/WoqalBc3OzNKaoqAhqtRrx8fGDnQqR3wumCubSx+tGoxGzZ89GbGws2trakJeXh9LSUhw4cAAajQaZmZnIzs5GREQE1Go1Vq1aBb1ej+nTpwMAUlJSEB8fj8WLF2PTpk2wWCzYsGEDDAYDVCqVRxZI5I96K1ig7IL1xaVVNTc3Y8mSJbh48SI0Gg0mT56MAwcO4LHHHgMAvPLKKwgJCUF6ejrsdjtSU1OxdetW6fFDhgxBQUEBVq5cCb1ejxEjRiAjIwO/+MUv3LsqogDQW8FMhrGwtQReAA36ez5y4Pd8KJhUlqixeVUsrH4QQF75ng8ReUeg7oIxfIj8QCDugjF8iPxAIO6CMXyI/EggVTCGD5GfCZQKxvAh8jOBUsEYPkR+yt8rGMOHyI/5cwVj+BD5MX+uYAwfogDgjxWM4UMUIPytgjF8iAKEv1Uwhg9RgPGXCsbwIQpA/lDBGD5EAcgfKhjDhyiA+XIFY/gQBThfrWAMH6IA56sVjOFDFCR8rYK5FD7btm3D5MmTpWtn6fV6FBYWSucfeeQRKBQKp2PFihVOz1FfX4+0tDQMHz4cWq0W69atQ3e3b/xhEAU6X6pgLv1G6jFjxmDjxo245557IITAzp07MW/ePBw/fhz33XcfAGDZsmVOV6MYPny49N89PT1IS0uDTqfD4cOHcfHiRSxZsgTDhg3Dr3/9azctiYhu5JsVTO5fSj/oq1dERERg8+bNyMzMxCOPPIL7778fr776ap9jCwsLMXfuXDQ2NkrXaN++fTvWr1+PS5cuQalU9us1efUKosH7uDzM7dcF88rVK3p6erBr1y50dHRIVyQFgLfffhujRo3CpEmTYDQa8fXXX0vnzGYzEhISpOABgNTUVNhsNtTW1t7wtex2O2w2m9NBRIMjdwVzOfJqamqg1+tx9epVhIaGIj8/X7rU8TPPPIOxY8ciOjoaJ06cwPr161FXV4f33nsPAGCxWJyCB4B022Kx3PA1TSYTXnrpJVenSkQ3IXcFc/nVJkyYgOrqalitVrz77rvIyMhAWVkZ4uPjsXz5cmlcQkICoqKiMHPmTJw9exZ33333gCdpNBqRnZ0t3bbZbIiJiRnw8xHRf8l1aWaXa5dSqcT48eORmJgIk8mEKVOm4LXXXutzbFJSEgDgzJkzAACdToempianMb23dTrdDV9TpVJJO2y9BxG5jxwVbNCf1jocDtjt9j7PVVdXAwCioqIAAHq9HjU1NWhubpbGFBUVQa1WS9WNiLxPji8iuvQey2g0Yvbs2YiNjUVbWxvy8vJQWlqKAwcO4OzZs8jLy8OcOXMwcuRInDhxAmvWrMGMGTMwefJkAEBKSgri4+OxePFibNq0CRaLBRs2bIDBYIBKpfLIAomo/7xZwVx659Pc3IwlS5ZgwoQJmDlzJiorK3HgwAE89thjUCqV+Ne//oWUlBTExcVh7dq1SE9Px759+6THDxkyBAUFBRgyZAj0ej2effZZLFmyxOl7QUQkL29VsEF/z0cO/J4PkedVlqhd3gXzyvd8iCiwefpnwRg+RHRDnqxgDB8iuiFP7oIxfIjoljxRwRg+RNQv7q5gDB8i6hd3VzCGDxG5xF0VjOFDRC5zRwVj+BCRy9xRwRg+RDRgg6lgDB8iGpSBVjCGDxENyjcrmPqO/geQfL+6fhB6fxbW1u6QeSZE1GvCA63I2nwFe//ff/+N3oxfhs/ly5cBAGP/57y8EyGiPrW1tUGj0dx0jF+GT0REBIBrFyC81QIDQe/vrG5oaAiKXyHL9fovIQTa2toQHR19y7F+GT4hIdc+qtJoNH7/l+WKYPv91Vyvf+rvGwJ+4ExEsmD4EJEs/DJ8VCoVXnzxxaD5pfNcb2ALtvX28svf4UxE/s8v3/kQkf9j+BCRLBg+RCQLhg8RyYLhQ0Sy8Mvw2bJlC+68807cdtttSEpKwtGjR+We0oCUl5fj8ccfR3R0NBQKBfbs2eN0XgiBnJwcREVF4fbbb0dycjI+//xzpzEtLS1YtGgR1Go1wsPDkZmZifb2di+uon9MJhOmTZuGsLAwaLVazJ8/H3V1dU5jrl69CoPBgJEjRyI0NBTp6eloampyGlNfX4+0tDQMHz4cWq0W69atQ3e3Zy/rOxDbtm3D5MmTpW8t6/V6FBYWSucDaa0DJvzMrl27hFKpFH/+859FbW2tWLZsmQgPDxdNTU1yT81lH3zwgfjpT38q3nvvPQFA5OfnO53fuHGj0Gg0Ys+ePeKTTz4RTzzxhBg3bpy4cuWKNGbWrFliypQp4siRI+LDDz8U48ePFwsXLvTySm4tNTVV7NixQ5w8eVJUV1eLOXPmiNjYWNHe3i6NWbFihYiJiRHFxcXi2LFjYvr06eI73/mOdL67u1tMmjRJJCcni+PHj4sPPvhAjBo1ShiNRjmWdFN79+4V//jHP8Rnn30m6urqxE9+8hMxbNgwcfLkSSFEYK11oPwufB588EFhMBik2z09PSI6OlqYTCYZZzV43w4fh8MhdDqd2Lx5s3Rfa2urUKlU4p133hFCCPHpp58KAKKyslIaU1hYKBQKhfjyyy+9NveBaG5uFgBEWVmZEOLa2oYNGyZ2794tjTl16pQAIMxmsxDiWliHhIQIi8Uijdm2bZtQq9XCbrd7dwEDcMcdd4g//vGPQbHW/vCr2tXZ2YmqqiokJydL94WEhCA5ORlms1nGmbnfuXPnYLFYnNaq0WiQlJQkrdVsNiM8PBwPPPCANCY5ORkhISGoqKjw+pxdYbVaAfz3NxRUVVWhq6vLab1xcXGIjY11Wm9CQgIiIyOlMampqbDZbKitrfXi7F3T09ODXbt2oaOjA3q9PqDX6gq/+qn2r776Cj09PU5/IQAQGRmJ06dPyzQrz7BYLADQ51p7z1ksFmi1WqfzQ4cORUREhDTGFzkcDqxevRoPPfQQJk2aBODaWpRKJcLDw53Gfnu9ff159J7zNTU1NdDr9bh69SpCQ0ORn5+P+Ph4VFdXB9xaB8KvwocCg8FgwMmTJ3Ho0CG5p+JREyZMQHV1NaxWK959911kZGSgrKxM7mn5DL+qXaNGjcKQIUOu2xVoamqCTqeTaVae0buem61Vp9OhubnZ6Xx3dzdaWlp89s8jKysLBQUFKCkpwZgxY6T7dTodOjs70dra6jT+2+vt68+j95yvUSqVGD9+PBITE2EymTBlyhS89tprAbnWgfCr8FEqlUhMTERxcbF0n8PhQHFxMfR6vYwzc79x48ZBp9M5rdVms6GiokJaq16vR2trK6qqqqQxBw8ehMPhQFJSktfnfDNCCGRlZSE/Px8HDx7EuHHjnM4nJiZi2LBhTuutq6tDfX2903pramqcAreoqAhqtRrx8fHeWcggOBwO2O32oFhrv8j9iberdu3aJVQqlcjNzRWffvqpWL58uQgPD3faFfAXbW1t4vjx4+L48eMCgPjd734njh8/Li5cuCCEuLbVHh4eLt5//31x4sQJMW/evD632qdOnSoqKirEoUOHxD333OOTW+0rV64UGo1GlJaWiosXL0rH119/LY1ZsWKFiI2NFQcPHhTHjh0Ter1e6PV66Xzv9nNKSoqorq4W+/fvF6NHj/bJ7ecXXnhBlJWViXPnzokTJ06IF154QSgUCvHPf/5TCBFYax0ovwsfIYR44403RGxsrFAqleLBBx8UR44ckXtKA1JSUiIAXHdkZGQIIa5tt//sZz8TkZGRQqVSiZkzZ4q6ujqn57h8+bJYuHChCA0NFWq1WixdulS0tbXJsJqb62udAMSOHTukMVeuXBE//vGPxR133CGGDx8unnzySXHx4kWn5zl//ryYPXu2uP3228WoUaPE2rVrRVdXl5dXc2s//OEPxdixY4VSqRSjR48WM2fOlIJHiMBa60Dx9/kQkSz86jMfIgocDB8ikgXDh4hkwfAhIlkwfIhIFgwfIpIFw4eIZMHwISJZMHyISBYMHyKSBcOHiGTxf0tdKxD8AFu3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perfect triangle means token at current step can only see tokens earlier in the autoregression sequence. \n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(masks[0].cpu());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the logits in a `forward` pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = self._model(query_responses, input_pos=position_ids, mask=masks)\n",
    "ref_logits = self._ref_model(query_responses, input_pos=position_ids, mask=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logits.shape[0] == grpo_size, 'The model forward pass is not making the correct number of GRPO samples.'\n",
    "assert logits.shape[1] <= self._max_generated_tokens, 'The model forward pass is not respecting max_generated_tokens.'\n",
    "assert logits.shape[2] == self._tokenizer.vocab_size - 1, 'The model forward pass is not respecting the vocab size.'\n",
    "\n",
    "assert ref_logits.shape[0] == grpo_size, 'The ref model forward pass is not making the correct number of GRPO samples.'\n",
    "assert ref_logits.shape[1] <= self._max_generated_tokens, 'The ref model forward pass is not respecting max_generated_tokens.'\n",
    "assert ref_logits.shape[2] == self._tokenizer.vocab_size - 1, 'The ref model forward pass is not respecting the vocab size.'\n",
    " \n",
    "assert logits.shape == ref_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate logprobs of the response using the current and reference policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[:, context_length - 1 :]\n",
    "logprobs = rlhf.batched_logits_to_logprobs(logits, responses, self._temperature)\n",
    "del logits\n",
    "torch.cuda.empty_cache()\n",
    "ref_logits = self._ref_model(\n",
    "    query_responses, input_pos=position_ids, mask=masks\n",
    ")\n",
    "ref_logits = rlhf.truncate_sequence_for_logprobs(ref_logits, context_length)\n",
    "ref_logprobs = rlhf.batched_logits_to_logprobs(\n",
    "    ref_logits, responses, self._temperature\n",
    ")\n",
    "del ref_logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logprobs.shape == ref_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace any tokens in the responses after the first stop token (usually EOS token) with padding\n",
    "# resulting in truncated responses\n",
    "(\n",
    "    response_padding_masks,\n",
    "    responses,\n",
    ") = rlhf.truncate_sequence_at_first_stop_token(  # [B x G, L]\n",
    "    responses, self._stop_token_ids, self._tokenizer.pad_id\n",
    ")\n",
    "\n",
    "responses = responses.reshape(batch_size, grpo_size, -1)  # [B, G, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fee54581a90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: compare to query_response_padding_masks above.\n",
    "plt.imshow(response_padding_masks.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He earns 12/60=$0.2/hour\n",
      "So, he earns 0.2 * 50 = $<<0.2*50=10>>10 in an hour\n",
      "So, he earns 10 * 2 = $<<10*2=20>>20\n",
      "An hour.\n",
      "So, he earns 20 * 60 = $<<20*60=1200>>1200 for babysitting for 6 hours.\n",
      "</think> <answer>1200</answer>\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "for i in range(grpo_size):\n",
    "    decoded = self._tokenizer.decode(responses[:, i, :].tolist()[0])\n",
    "    print(decoded)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing rewards and successes\n",
    "\n",
    "Since the reward is obviously the interesting part here, lets implement it inline.\n",
    "Note that this is a signficant deviation from how rewards are being computed in the current `/dev` implementation.\n",
    "RL nerd-sniped me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copied & slightly refactored from torchtune.dev.grpo.rewards import batch_shaped_correctness_reward.\n",
    "### Key changes\n",
    "# 1. Seperate reward function definition from reward batch processing logic.\n",
    "# 2. Slightly modified XML tag parsing.\n",
    "# 3. Reduce potential for false negative from formatting differences.\n",
    "# 4. Explicitly reward step-by-step reasoning.\n",
    "# 5. Option to return rewards with breakdown by component.\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union, Any\n",
    "import re\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from torchtune.modules.transforms.tokenizers import ModelTokenizer\n",
    "\n",
    "\n",
    "def extract_tags(text: str) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Parse XML-like tags from text. Returns a dictionary with keys 'think' and 'answer'.\n",
    "    The values are lists of strings, with each string being the content of a tag.\n",
    "    \"\"\"\n",
    "    cleaned_text = text.replace(\"<<\", \"\").replace(\">>\", \"\")\n",
    "    cleaned_text = re.sub(r'<\\s*(\\/?)\\s*(think|answer)\\s*>', r'<\\1\\2>', cleaned_text)\n",
    "    xml_string = f\"<root>{cleaned_text}</root>\"\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string)\n",
    "        return {\n",
    "            \"think\": [\n",
    "                elem.text if elem.text is not None else \"\" for elem in root.findall(\"think\")\n",
    "            ],\n",
    "            \"answer\": [\n",
    "                elem.text if elem.text is not None else \"\"\n",
    "                for elem in root.findall(\"answer\")\n",
    "            ],\n",
    "        }\n",
    "    except ET.ParseError:\n",
    "        # Fall back to regex-based extraction if XML parsing fails\n",
    "        think_tags = re.findall(r'<think>(.*?)</think>', cleaned_text, re.DOTALL)\n",
    "        answer_tags = re.findall(r'<answer>(.*?)</answer>', cleaned_text, re.DOTALL)\n",
    "        return {\n",
    "            \"think\": think_tags,\n",
    "            \"answer\": answer_tags\n",
    "        }\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize answer string to handle common variations.\n",
    "    - Remove currency symbols, commas, and extra whitespace\n",
    "    - Standardize number format\n",
    "    \"\"\"\n",
    "    # Remove currency symbols, commas in numbers, and normalize whitespace\n",
    "    normalized = re.sub(r'[\\$,\\s]', '', answer)\n",
    "    \n",
    "    # Try to extract numbers if the answer is primarily numeric\n",
    "    number_match = re.search(r'[-+]?\\d*\\.?\\d+', normalized)\n",
    "    if number_match and len(number_match.group()) > len(normalized) * 0.5:\n",
    "        return number_match.group()\n",
    "    \n",
    "    return normalized.strip().lower()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardComponentInfo:\n",
    "    \"\"\"Metadata for a reward component.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    max_value: float\n",
    "    weight: float = 1.0\n",
    "    is_critical: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardResult:\n",
    "    \"\"\"Result of evaluating a reward function.\"\"\"\n",
    "    raw_score: float\n",
    "    weighted_score: float\n",
    "    max_possible: float\n",
    "    is_fully_satisfied: bool = False\n",
    "    info: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class RewardComponent:\n",
    "    \"\"\"Base class for reward components.\"\"\"\n",
    "\n",
    "    def __init__(self, info: RewardComponentInfo):\n",
    "        self.info = info \n",
    "    \n",
    "    def evaluate(self, context: Dict[str, Any], debug:bool=False) -> RewardResult:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class FormatComponent(RewardComponent):\n",
    "    \"\"\"Reward proper formatting with <think> and <answer> tags.\"\"\"\n",
    "\n",
    "    def evaluate(self, context: Dict[str, Any]) -> RewardResult:\n",
    "        tags = context['tags']\n",
    "        score = 0.0\n",
    "        info = {}\n",
    "        if len(tags[\"answer\"]) == 1: \n",
    "            answer_score = self.info.max_value / 2\n",
    "            info[\"answer_tag\"] = \"perfect\"\n",
    "        elif len(tags[\"answer\"]) > 1:\n",
    "            answer_score = self.info.max_value / 5 # some credit for having multiple <answer> tags.\n",
    "            info[\"answer_tag\"] = \"multiple\"\n",
    "        else:\n",
    "            answer_score = 0.0\n",
    "            info[\"answer_tag\"] = \"missing\"\n",
    "        if len(tags[\"think\"]) == 1:\n",
    "            think_score = self.info.max_value / 2\n",
    "            info[\"think_tag\"] = \"perfect\"\n",
    "        elif len(tags[\"think\"]) > 0:\n",
    "            think_score = self.info.max_value / 5\n",
    "            info[\"think_tag\"] = \"multiple\"\n",
    "        else:\n",
    "            think_score = 0.0\n",
    "            info[\"think_tag\"] = \"missing\"\n",
    "\n",
    "        score = answer_score + think_score\n",
    "        fully_satistifed = len(tags[\"answer\"]) == 1 and len(tags[\"think\"]) == 1\n",
    "\n",
    "        return RewardResult(\n",
    "            raw_score=score,\n",
    "            weighted_score=score * self.info.weight,\n",
    "            max_possible=self.info.max_value * self.info.weight,\n",
    "            is_fully_satisfied=fully_satistifed,\n",
    "            info=info\n",
    "        )\n",
    "    \n",
    "class ExactAnswerComponent(RewardComponent):\n",
    "    \"\"\"Reward component for exact answer match.\"\"\"\n",
    "    \n",
    "    def evaluate(self, context: Dict[str, Any]) -> RewardResult:\n",
    "        tags = context['tags']\n",
    "        normalized_ground_truth = context['normalized_ground_truth']\n",
    "        \n",
    "        normalized_answers = [normalize_answer(a) for a in tags[\"answer\"]]\n",
    "        \n",
    "        # Check for exact match\n",
    "        exact_match = normalized_ground_truth in normalized_answers\n",
    "        \n",
    "        score = self.info.max_value if exact_match else 0.0\n",
    "\n",
    "        info = {\n",
    "            \"exact_match\": exact_match,\n",
    "            \"normalized_answers\": normalized_answers,\n",
    "            \"normalized_ground_truth\": normalized_ground_truth\n",
    "        }\n",
    "        \n",
    "        return RewardResult(\n",
    "            raw_score=score,\n",
    "            weighted_score=score * self.info.weight,\n",
    "            max_possible=self.info.max_value * self.info.weight,\n",
    "            is_fully_satisfied=exact_match,\n",
    "            info=info\n",
    "        )\n",
    "\n",
    "\n",
    "class SubstringAnswerComponent(RewardComponent):\n",
    "    \"\"\"Reward component for substring match in answer.\"\"\"\n",
    "    \n",
    "    def evaluate(self, context: Dict[str, Any]) -> RewardResult:\n",
    "        tags = context['tags']\n",
    "        normalized_ground_truth = context['normalized_ground_truth']\n",
    "        \n",
    "        normalized_answers = [normalize_answer(a) for a in tags[\"answer\"]]\n",
    "        \n",
    "        # Check for substring match\n",
    "        substring_match = any(normalized_ground_truth in norm_ans for norm_ans in normalized_answers)\n",
    "        \n",
    "        score = self.info.max_value if substring_match else 0.0\n",
    "\n",
    "        info = {\n",
    "            \"substring_match\": substring_match,\n",
    "            \"normalized_answers\": normalized_answers,\n",
    "            \"normalized_ground_truth\": normalized_ground_truth\n",
    "        }\n",
    "        \n",
    "        return RewardResult(\n",
    "            raw_score=score,\n",
    "            weighted_score=score * self.info.weight,\n",
    "            max_possible=self.info.max_value * self.info.weight,\n",
    "            is_fully_satisfied=substring_match,\n",
    "            info=info\n",
    "        )\n",
    "\n",
    "\n",
    "class ReasoningQualityComponent(RewardComponent):\n",
    "    \"\"\"Reward component for evaluating quality of reasoning.\"\"\"\n",
    "    \n",
    "    def evaluate(self, context: Dict[str, Any]) -> RewardResult:\n",
    "        tags = context['tags']\n",
    "        answer = context['answer']\n",
    "        \n",
    "        if not tags[\"think\"]:\n",
    "            return RewardResult(\n",
    "                raw_score=0.0,\n",
    "                weighted_score=0.0,\n",
    "                max_possible=self.info.max_value * self.info.weight,\n",
    "                is_fully_satisfied=False,\n",
    "                info={\"reasoning\": \"missing\"}\n",
    "            )\n",
    "        \n",
    "        thinking = tags[\"think\"][0]\n",
    "        score = 0.0\n",
    "        info = {}\n",
    "        \n",
    "        # Reward for having calculation steps (numbers and operators)\n",
    "        calc_pattern = r'\\d+\\s*[\\+\\-\\*\\/]\\s*\\d+'\n",
    "        calculations = re.findall(calc_pattern, thinking)\n",
    "        calc_score = min(len(calculations), 5) * (self.info.max_value / 10)  # Up to half points for calculations\n",
    "        info[\"calculations\"] = len(calculations)\n",
    "        \n",
    "        # Check if final calculation leads to answer\n",
    "        normalized_answer = normalize_answer(answer)\n",
    "        answer_in_reasoning = normalized_answer in thinking\n",
    "        if answer_in_reasoning:\n",
    "            answer_score = 3.0 * (self.info.max_value / 10)  # 30% of max for answer in reasoning\n",
    "        else:\n",
    "            answer_score = 0.0\n",
    "        info[\"answer_in_reasoning\"] = answer_in_reasoning\n",
    "        \n",
    "        # Check for logical progression (consistent step-by-step approach)\n",
    "        step_pattern = re.search(r'(first|1st|step 1|begin|start).*?(then|next|2nd|step 2)', \n",
    "                              thinking, re.IGNORECASE|re.DOTALL)\n",
    "        if step_pattern:\n",
    "            step_score = 2.0 * (self.info.max_value / 10)  # 20% of max for step-by-step reasoning\n",
    "            info[\"step_reasoning\"] = True\n",
    "        else:\n",
    "            step_score = 0.0\n",
    "            info[\"step_reasoning\"] = False\n",
    "            \n",
    "        score = calc_score + answer_score + step_score\n",
    "        fully_satisfied = score >= self.info.max_value * 0.8\n",
    "        \n",
    "        return RewardResult(\n",
    "            raw_score=score,\n",
    "            weighted_score=score * self.info.weight,\n",
    "            max_possible=self.info.max_value * self.info.weight,\n",
    "            is_fully_satisfied=fully_satisfied,\n",
    "            info=info\n",
    "        )\n",
    "\n",
    "\n",
    "class CorrectFinalAnswerComponent(RewardComponent):\n",
    "    \"\"\"Critical component that gives full reward if final answer is correct.\"\"\"\n",
    "    \n",
    "    def evaluate(self, context: Dict[str, Any]) -> RewardResult:\n",
    "        tags = context['tags']\n",
    "        normalized_ground_truth = context['normalized_ground_truth']\n",
    "        \n",
    "        if not tags[\"answer\"]:\n",
    "            return RewardResult(\n",
    "                raw_score=0.0,\n",
    "                weighted_score=0.0,\n",
    "                max_possible=self.info.max_value * self.info.weight,\n",
    "                is_fully_satisfied=False,\n",
    "                info={\"final_answer\": \"missing\"}\n",
    "            )\n",
    "        \n",
    "        final_answer = normalize_answer(tags[\"answer\"][-1])\n",
    "        correct_final_answer = final_answer == normalized_ground_truth\n",
    "        \n",
    "        score = self.info.max_value if correct_final_answer else 0.0\n",
    "        info = {\n",
    "            \"correct_final_answer\": correct_final_answer,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"normalized_ground_truth\": normalized_ground_truth\n",
    "        }\n",
    "        \n",
    "        return RewardResult(\n",
    "            raw_score=score,\n",
    "            weighted_score=score * self.info.weight,\n",
    "            max_possible=self.info.max_value * self.info.weight,\n",
    "            is_fully_satisfied=correct_final_answer,\n",
    "            info=info\n",
    "        )\n",
    "\n",
    "\n",
    "class RewardFunction:\n",
    "    \"\"\"\n",
    "    Aggregates multiple reward components and evaluates completions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, components: List[RewardComponent] = None):\n",
    "        self.components = components or []\n",
    "        self._setup_default_components()\n",
    "    \n",
    "    def _setup_default_components(self):\n",
    "        \"\"\"Set up the default reward components if none provided.\"\"\"\n",
    "        if not self.components:\n",
    "            self.components = [\n",
    "                FormatComponent(\n",
    "                    RewardComponentInfo(\n",
    "                        name=\"format\",\n",
    "                        description=\"Proper format with single think and answer tags\",\n",
    "                        max_value=10.0,\n",
    "                        weight=1.0\n",
    "                    )\n",
    "                ),\n",
    "                ExactAnswerComponent(\n",
    "                    RewardComponentInfo(\n",
    "                        name=\"exact_answer\",\n",
    "                        description=\"Exact match of answer in any answer tag\",\n",
    "                        max_value=20.0,\n",
    "                        weight=1.0\n",
    "                    )\n",
    "                ),\n",
    "                SubstringAnswerComponent(\n",
    "                    RewardComponentInfo(\n",
    "                        name=\"substring_answer\",\n",
    "                        description=\"Substring match of answer in any answer tag\",\n",
    "                        max_value=10.0,\n",
    "                        weight=1.0\n",
    "                    )\n",
    "                ),\n",
    "                ReasoningQualityComponent(\n",
    "                    RewardComponentInfo(\n",
    "                        name=\"reasoning\",\n",
    "                        description=\"Quality of reasoning in think tag\",\n",
    "                        max_value=10.0,\n",
    "                        weight=0.6\n",
    "                    )\n",
    "                ),\n",
    "                CorrectFinalAnswerComponent(\n",
    "                    RewardComponentInfo(\n",
    "                        name=\"correct_final_answer\",\n",
    "                        description=\"Correct answer in final answer tag\",\n",
    "                        max_value=100.0,\n",
    "                        weight=1.0,\n",
    "                        is_critical=True\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "    \n",
    "    def evaluate(\n",
    "        self, \n",
    "        answer: str, \n",
    "        completion: str,\n",
    "        return_details: bool = False\n",
    "    ) -> Union[Tuple[float, float], Tuple[float, float, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Evaluate a completion against an answer.\n",
    "        \n",
    "        Args:\n",
    "            answer: The ground truth answer.\n",
    "            completion: The model's (decoded) completion.\n",
    "            return_details: Whether to return detailed component scores.\n",
    "            \n",
    "        Returns:\n",
    "            reward: Total reward value.\n",
    "            success: Binary success indicator (1.0 or 0.0).\n",
    "            details: (optional) Dictionary of component details.\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = extract_tags(\"<think>\" + completion)\n",
    "        normalized_ground_truth = normalize_answer(answer)\n",
    "        \n",
    "        context = {\n",
    "            'answer': answer,\n",
    "            'completion': completion,\n",
    "            'tags': tags,\n",
    "            'normalized_ground_truth': normalized_ground_truth\n",
    "        }\n",
    "        \n",
    "        component_results = {}\n",
    "        total_reward = 0.0\n",
    "        success = 0.0\n",
    "        \n",
    "        for component in self.components:\n",
    "            result = component.evaluate(context)\n",
    "            component_results[component.info.name] = result\n",
    "            \n",
    "            total_reward += result.weighted_score\n",
    "            \n",
    "            if component.info.is_critical and result.is_fully_satisfied:\n",
    "                total_reward = 100.0  # Override with full reward\n",
    "                success = 1.0\n",
    "        \n",
    "        if return_details:\n",
    "            details = {\n",
    "                'component_results': component_results,\n",
    "                'total_reward': total_reward,\n",
    "                'success': success\n",
    "            }\n",
    "            return total_reward, success, details\n",
    "        \n",
    "        return total_reward, success\n",
    "\n",
    "\n",
    "def batch_shaped_correctness_reward(\n",
    "    tokenizer: ModelTokenizer, \n",
    "    completions: torch.Tensor, \n",
    "    answers: list[str],\n",
    "    reward_function: Optional[RewardFunction] = None,\n",
    "    return_details: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Utility function to apply the shaped reward function to a GRPO-style batch of completions.\"\"\"\n",
    "\n",
    "    if reward_function is None:\n",
    "        reward_function = RewardFunction()\n",
    "\n",
    "    if return_details:\n",
    "        all_details = []\n",
    "\n",
    "    batch_size, grpo_size, *_ = completions.shape\n",
    "    rewards = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "    successes = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "    # completions :: [B, G, L]\n",
    "    for b in range(batch_size):\n",
    "        for g in range(grpo_size):\n",
    "            text_completion = tokenizer.decode(\n",
    "                completions[b, g].tolist()\n",
    "            )  # skips special tokens, stops at eos\n",
    "            if return_details:\n",
    "                reward, success, details = reward_function.evaluate(\n",
    "                    answer=answers[b],\n",
    "                    completion=text_completion,\n",
    "                    return_details=True\n",
    "                ) \n",
    "                all_details.append(details)\n",
    "            else:\n",
    "                reward, success = reward_function.evaluate(\n",
    "                    answer=answers[b],\n",
    "                    completion=text_completion\n",
    "                )\n",
    "            rewards[b, g] = reward\n",
    "            successes[b ,g] = success\n",
    "\n",
    "    if return_details:\n",
    "        return rewards, successes, all_details\n",
    "    \n",
    "    return rewards, successes, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with default settings\n",
    "rewards, successes, details = batch_shaped_correctness_reward(\n",
    "    self._tokenizer, responses, answers, return_details=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100.0, 0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[successes == 1].mean().item(), 0. if not rewards.shape[0] > 1 else rewards[successes == 1].std() \n",
    "# NOTE: 100, 0 for reward func above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = rewards.to(self._device)\n",
    "successes = successes.to(self._device)\n",
    "\n",
    "advantages = (rewards - rewards.mean(1, keepdim=True)) / (\n",
    "    rewards.std(1, keepdim=True) + 1e-4\n",
    ")\n",
    "advantages = advantages.reshape(batch_size * grpo_size) # flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses(prompt, responses, tokenizer, grpo_size, advantages=None, rewards=None, successes=None, component_details=None, show_n:int=None):\n",
    "    \"\"\"\n",
    "    Display responses in Jupyter with reward metrics when available.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The tensor of prompt tokens\n",
    "        responses: The tensor of response tokens\n",
    "        tokenizer: The tokenizer to decode responses\n",
    "        grpo_size: Number of responses to print\n",
    "        advantages: Optional tensor of advantage values [batch_size, grpo_size]. NOTE: current implementation assumes batch_size=1.\n",
    "        rewards: Optional tensor of reward values [batch_size, grpo_size]. NOTE: current implementation assumes batch_size=1.\n",
    "        successes: Optional tensor of success indicators [batch_size, grpo_size]\n",
    "        component_details: Optional list of component-level reward details\n",
    "    \n",
    "    Returns:\n",
    "        str that can be passed to IPython HTML display, embedded in Metaflow card or any other UI, of responses with optional reward metrics.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    if rewards.shape[0] > 1:\n",
    "        raise Warning('Rewards dim 0 is > 1, meaning batch_size > 1. This function only displays the first member of the batch.')\n",
    "    \n",
    "    html_output = \"\"\"\n",
    "        <style>\n",
    "            .prompt-box {\n",
    "                margin: 20px 0;\n",
    "                padding: 15px;\n",
    "                border: 1px solid #C4C7AC;\n",
    "                border-radius: 8px;\n",
    "                background-color: #F0EBE5;\n",
    "                color: #4A4A67;\n",
    "                font-family: 'Courier New', monospace;\n",
    "                font-size: 14px;\n",
    "                line-height: 1.6;\n",
    "                white-space: pre-wrap;\n",
    "                word-wrap: break-word;\n",
    "            }\n",
    "            .response-container {\n",
    "                margin: 20px 0;\n",
    "                border: 1px solid #C4C7AC;\n",
    "                border-radius: 8px;\n",
    "                overflow: hidden;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
    "                font-family: 'Courier New', monospace;\n",
    "                max-width: 100%;\n",
    "            }\n",
    "            .response-header {\n",
    "                background-color: #F0EBE5;\n",
    "                padding: 10px 15px;\n",
    "                font-size: 16px;\n",
    "                font-weight: bold;\n",
    "                border-bottom: 1px solid #C4C7AC;\n",
    "                color: #4A4A67;\n",
    "                display: flex;\n",
    "                justify-content: space-between;\n",
    "                align-items: center;\n",
    "            }\n",
    "            .response-body {\n",
    "                background-color: #ffffff;\n",
    "                color: #4A4A67;\n",
    "                padding: 15px;\n",
    "                white-space: pre-wrap;\n",
    "                word-wrap: break-word;\n",
    "                line-height: 1.6;\n",
    "                font-size: 14px;\n",
    "            }\n",
    "            .think-tag {\n",
    "                color: #BE6A1A;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .answer-tag {\n",
    "                color: #2C6846;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .calculation {\n",
    "                color: #2E5CA7;\n",
    "                background-color: #E9F0FA;\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 2px;\n",
    "            }\n",
    "            .math {\n",
    "                font-family: 'Courier New', monospace;\n",
    "                background-color: #F0EBE5;\n",
    "                padding: 0 3px;\n",
    "                border-radius: 3px;\n",
    "            }\n",
    "            .metric-label {\n",
    "                color: #4A4A67;\n",
    "            }\n",
    "            .metrics-container {\n",
    "                background-color: #F0EBE5;\n",
    "                border-top: 1px solid #C4C7AC;\n",
    "                padding: 10px 15px;\n",
    "            }\n",
    "            .metric-score {\n",
    "                font-family: monospace;\n",
    "                font-weight: bold;\n",
    "                padding: 2px 8px;\n",
    "                border-radius: 4px;\n",
    "                display: inline-block;\n",
    "                margin-right: 8px;\n",
    "            }\n",
    "            .score-high {\n",
    "                background-color: #D3EFE0;\n",
    "                color: #177350;\n",
    "            }\n",
    "            .score-medium {\n",
    "                background-color: #FCF1D6;\n",
    "                color: #BE6A1A;\n",
    "            }\n",
    "            .score-low {\n",
    "                background-color: #FAD9D8;\n",
    "                color: #C5393A;\n",
    "            }\n",
    "            .success-badge {\n",
    "                background-color: #177350;\n",
    "                color: white;\n",
    "                padding: 3px 8px;\n",
    "                border-radius: 4px;\n",
    "                font-size: 12px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .failure-badge {\n",
    "                background-color: #4A4A67;\n",
    "                color: white;\n",
    "                padding: 3px 8px;\n",
    "                border-radius: 4px;\n",
    "                font-size: 12px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .component-table {\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                margin-top: 10px;\n",
    "                font-size: 13px;\n",
    "            }\n",
    "            .component-table th {\n",
    "                background-color: #F0EBE5;\n",
    "                text-align: left;\n",
    "                padding: 6px 10px;\n",
    "                color: #4A4A67;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .component-table td {\n",
    "                border-top: 1px solid #C4C7AC;\n",
    "                padding: 6px 10px;\n",
    "                color: #4A4A67;\n",
    "            }\n",
    "            .component-progress {\n",
    "                height: 8px;\n",
    "                width: 100%;\n",
    "                background-color: #E5E7D9;\n",
    "                border-radius: 4px;\n",
    "                overflow: hidden;\n",
    "            }\n",
    "            .component-progress-bar {\n",
    "                height: 100%;\n",
    "                background-color: #6B9BD0;\n",
    "            }\n",
    "            .metrics-toggle {\n",
    "                cursor: pointer;\n",
    "                color: #3F7DC9;\n",
    "                text-decoration: underline;\n",
    "                font-size: 12px;\n",
    "                margin-top: 5px;\n",
    "                display: inline-block;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .details-container {\n",
    "                display: none;\n",
    "                margin-top: 10px;\n",
    "                border-top: 1px solid #C4C7AC;\n",
    "                padding-top: 10px;\n",
    "            }\n",
    "    </style>\n",
    "    <script>\n",
    "    function toggleDetails(id) {\n",
    "        var details = document.getElementById('details-' + id);\n",
    "        if (details.style.display === 'none' || details.style.display === '') {\n",
    "            details.style.display = 'block';\n",
    "        } else {\n",
    "            details.style.display = 'none';\n",
    "        }\n",
    "    }\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    has_rewards = rewards is not None and successes is not None\n",
    "    has_details = component_details is not None\n",
    "\n",
    "    ### PROCESS PROMPTS ###\n",
    "    if hasattr(prompt, 'shape'):\n",
    "        prompt_decoded = tokenizer.decode(\n",
    "            prompt[0].tolist() \n",
    "            if len(prompt.shape) > 0 and prompt.shape[0] > 0 else prompt.tolist()\n",
    "        )\n",
    "    else:\n",
    "        prompt_decoded = str(prompt)\n",
    "    \n",
    "    ### PROCESS RESPONSES ###\n",
    "    responses_decoded = []\n",
    "    for i in range(grpo_size):\n",
    "        try:\n",
    "            decoded = tokenizer.decode(responses[:, i, :].tolist()[0])\n",
    "            responses_decoded.append(decoded)\n",
    "        except:\n",
    "            responses_decoded.append(\"Sample response\")\n",
    "    \n",
    "    if show_n:\n",
    "        responses_decoded = responses_decoded[:show_n]\n",
    "    \n",
    "    ### SHOW PROMPT ###\n",
    "    formatted_prompt = prompt_decoded.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "    formatted_prompt = formatted_prompt.replace(\"\\n\", \"<br>\")\n",
    "    html_output += f'<div class=\"prompt-box\"><div style=\"background-color: #F0EBE5; font-size: 16px; font-weight: bold;\">Prompt</div>{formatted_prompt}</div>'\n",
    "\n",
    "    for i, response in enumerate(responses_decoded):\n",
    "        html_output += f'<div class=\"response-container\">'\n",
    "        ### START HEADER ###\n",
    "        html_output += f'<div class=\"response-header\">'\n",
    "        html_output += f'<div>Response #{i+1}</div>'\n",
    "        if has_rewards:\n",
    "            reward = rewards[0][i].item()  # Assuming batch_size=1 for simplicity\n",
    "            success = successes[0][i].item()\n",
    "            if success > 0.5:\n",
    "                html_output += f'<div class=\"success-badge\">SUCCESS</div>'\n",
    "            else:\n",
    "                html_output += f'<div class=\"failure-badge\">FAIL</div>'\n",
    "        html_output += '</div>'\n",
    "        ### START RESPONSE BODY ### \n",
    "        html_output += f'<div class=\"response-body\">'\n",
    "        response = response.replace(\"\\n\", \"<br>\")\n",
    "        # Escape HTML tags but keep <br> tags.\n",
    "        response = response.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "        response = response.replace(\"&lt;br&gt;\", \"<br>\")\n",
    "        ### HIGHLIGHT THINK TAGS ###\n",
    "        response = re.sub(\n",
    "            r'&lt;/think&gt;', \n",
    "            '<span class=\"think-tag\">&lt;/think&gt;</span>', \n",
    "            response\n",
    "        )\n",
    "        response = re.sub(\n",
    "            r'&lt;think&gt;', \n",
    "            '<span class=\"think-tag\">&lt;think&gt;</span>', \n",
    "            response\n",
    "        )\n",
    "        ### HIGHLIGHT ANSWER TAG ###\n",
    "        response = re.sub(\n",
    "            r'&lt;/answer&gt;', \n",
    "            '<span class=\"answer-tag\">&lt;/answer&gt;</span>', \n",
    "            response\n",
    "        )\n",
    "        response = re.sub(\n",
    "            r'&lt;answer&gt;', \n",
    "            '<span class=\"answer-tag\">&lt;answer&gt;</span>', \n",
    "            response\n",
    "        )\n",
    "        # HIGHLIGHT CALCULATIONS with $<<...>>$ pattern ###\n",
    "        response = re.sub(\n",
    "            r'\\$&lt;&lt;(.+?)&gt;&gt;\\$', \n",
    "            r'<span class=\"calculation\">$&lt;&lt;\\1&gt;&gt;$</span>', \n",
    "            response\n",
    "        )\n",
    "        ### FORMAT MATH EXPRESSIONS ###\n",
    "        response = re.sub(\n",
    "            r'(\\d+[*/+-]\\d+(?:[*/+-]\\d+)*)', \n",
    "            r'<span class=\"math\">\\1</span>', \n",
    "            response\n",
    "        )\n",
    "        html_output += response\n",
    "        html_output += '</div>' \n",
    "        ### START METRICS ###\n",
    "        if has_rewards:\n",
    "            html_output += f'<div class=\"metrics-container\">'\n",
    "            reward = rewards[0][i].item()  # Assuming batch_size=1\n",
    "            advantage = advantages[i].item()\n",
    "            score_class = \"score-high\" if reward >= 80 else \"score-medium\" if reward >= 30 else \"score-low\"\n",
    "            html_output += f'<div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score {score_class}\">{reward:.1f}</span></div>'\n",
    "            html_output += f'<div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score {score_class}\">{advantage:.1f}</span></div>'\n",
    "            if has_details and i < len(component_details):\n",
    "                details = component_details[i]\n",
    "                html_output += f'<a class=\"metrics-toggle\" onclick=\"toggleDetails({i})\">Show component details</a>'\n",
    "                html_output += f'<div id=\"details-{i}\" class=\"details-container\">'\n",
    "                ### TABLE OF COMPONENTS ###\n",
    "                html_output += '<table class=\"component-table\">'\n",
    "                html_output += '<tr><th>Component</th><th>Score</th><th>Weight</th><th>Contribution</th></tr>'\n",
    "                for comp_name, result in details['component_results'].items():\n",
    "                    raw_score = result.raw_score\n",
    "                    weighted_score = result.weighted_score\n",
    "                    max_possible = result.max_possible\n",
    "                    is_satisfied = result.is_fully_satisfied\n",
    "                    # For progress bar.\n",
    "                    percentage = (weighted_score / max_possible) * 100 if max_possible > 0 else 0\n",
    "                    # Format row in table. Another SO to Claude.\n",
    "                    status_color = \"#2b8a3e\" if is_satisfied else \"#e67700\"\n",
    "                    html_output += f'<tr>'\n",
    "                    html_output += f'<td style=\"color: {status_color}; font-weight: {(\"bold\" if is_satisfied else \"normal\")}\">{comp_name}</td>'\n",
    "                    max_raw = max_possible/result.weighted_score*raw_score if raw_score > 0 and result.weighted_score > 0 else max_possible\n",
    "                    html_output += f'<td>{raw_score:.1f} / {max_raw:.1f}</td>'\n",
    "                    html_output += f'<td>{result.weighted_score/raw_score if raw_score > 0 else 0:.1f}x</td>'\n",
    "                    html_output += f'<td>'\n",
    "                    html_output += f'<div class=\"component-progress\">'\n",
    "                    html_output += f'<div class=\"component-progress-bar\" style=\"width: {min(percentage, 100)}%; background-color: {status_color if is_satisfied else \"#4dabf7\"};\"></div>'\n",
    "                    html_output += f'</div>'\n",
    "                    html_output += f'</td>'\n",
    "                    html_output += f'</tr>'\n",
    "                html_output += '</table>'\n",
    "                # Show component by component \n",
    "                for comp_name, result in details['component_results'].items():\n",
    "                    if hasattr(result, 'debug_info') and result.debug_info:\n",
    "                        html_output += f'<div style=\"margin-top: 8px; font-size: 12px; color: #495057;\">'\n",
    "                        html_output += f'<strong>{comp_name} details:</strong> '            \n",
    "                        # Format debug info\n",
    "                        debug_parts = []\n",
    "                        for key, value in result.debug_info.items():\n",
    "                            if isinstance(value, bool):\n",
    "                                icon = \"âœ“\" if value else \"âœ—\"\n",
    "                                color = \"#2b8a3e\" if value else \"#c92a2a\"\n",
    "                                debug_parts.append(f'<span style=\"color: {color}\">{key}: {icon}</span>')\n",
    "                            else:\n",
    "                                debug_parts.append(f'{key}: {value}')\n",
    "                        html_output += ', '.join(debug_parts)\n",
    "                        html_output += '</div>'\n",
    "                html_output += '</div>'  # Close details container\n",
    "            html_output += '</div>'  # Close metrics container\n",
    "        html_output += '</div>'  # Close response container\n",
    "    \n",
    "    return html_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .prompt-box {\n",
       "                margin: 20px 0;\n",
       "                padding: 15px;\n",
       "                border: 1px solid #C4C7AC;\n",
       "                border-radius: 8px;\n",
       "                background-color: #F0EBE5;\n",
       "                color: #4A4A67;\n",
       "                font-family: 'Courier New', monospace;\n",
       "                font-size: 14px;\n",
       "                line-height: 1.6;\n",
       "                white-space: pre-wrap;\n",
       "                word-wrap: break-word;\n",
       "            }\n",
       "            .response-container {\n",
       "                margin: 20px 0;\n",
       "                border: 1px solid #C4C7AC;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
       "                font-family: 'Courier New', monospace;\n",
       "                max-width: 100%;\n",
       "            }\n",
       "            .response-header {\n",
       "                background-color: #F0EBE5;\n",
       "                padding: 10px 15px;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                border-bottom: 1px solid #C4C7AC;\n",
       "                color: #4A4A67;\n",
       "                display: flex;\n",
       "                justify-content: space-between;\n",
       "                align-items: center;\n",
       "            }\n",
       "            .response-body {\n",
       "                background-color: #ffffff;\n",
       "                color: #4A4A67;\n",
       "                padding: 15px;\n",
       "                white-space: pre-wrap;\n",
       "                word-wrap: break-word;\n",
       "                line-height: 1.6;\n",
       "                font-size: 14px;\n",
       "            }\n",
       "            .think-tag {\n",
       "                color: #BE6A1A;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .answer-tag {\n",
       "                color: #2C6846;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .calculation {\n",
       "                color: #2E5CA7;\n",
       "                background-color: #E9F0FA;\n",
       "                padding: 2px 4px;\n",
       "                border-radius: 2px;\n",
       "            }\n",
       "            .math {\n",
       "                font-family: 'Courier New', monospace;\n",
       "                background-color: #F0EBE5;\n",
       "                padding: 0 3px;\n",
       "                border-radius: 3px;\n",
       "            }\n",
       "            .metric-label {\n",
       "                color: #4A4A67;\n",
       "            }\n",
       "            .metrics-container {\n",
       "                background-color: #F0EBE5;\n",
       "                border-top: 1px solid #C4C7AC;\n",
       "                padding: 10px 15px;\n",
       "            }\n",
       "            .metric-score {\n",
       "                font-family: monospace;\n",
       "                font-weight: bold;\n",
       "                padding: 2px 8px;\n",
       "                border-radius: 4px;\n",
       "                display: inline-block;\n",
       "                margin-right: 8px;\n",
       "            }\n",
       "            .score-high {\n",
       "                background-color: #D3EFE0;\n",
       "                color: #177350;\n",
       "            }\n",
       "            .score-medium {\n",
       "                background-color: #FCF1D6;\n",
       "                color: #BE6A1A;\n",
       "            }\n",
       "            .score-low {\n",
       "                background-color: #FAD9D8;\n",
       "                color: #C5393A;\n",
       "            }\n",
       "            .success-badge {\n",
       "                background-color: #177350;\n",
       "                color: white;\n",
       "                padding: 3px 8px;\n",
       "                border-radius: 4px;\n",
       "                font-size: 12px;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .failure-badge {\n",
       "                background-color: #4A4A67;\n",
       "                color: white;\n",
       "                padding: 3px 8px;\n",
       "                border-radius: 4px;\n",
       "                font-size: 12px;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .component-table {\n",
       "                width: 100%;\n",
       "                border-collapse: collapse;\n",
       "                margin-top: 10px;\n",
       "                font-size: 13px;\n",
       "            }\n",
       "            .component-table th {\n",
       "                background-color: #F0EBE5;\n",
       "                text-align: left;\n",
       "                padding: 6px 10px;\n",
       "                color: #4A4A67;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .component-table td {\n",
       "                border-top: 1px solid #C4C7AC;\n",
       "                padding: 6px 10px;\n",
       "                color: #4A4A67;\n",
       "            }\n",
       "            .component-progress {\n",
       "                height: 8px;\n",
       "                width: 100%;\n",
       "                background-color: #E5E7D9;\n",
       "                border-radius: 4px;\n",
       "                overflow: hidden;\n",
       "            }\n",
       "            .component-progress-bar {\n",
       "                height: 100%;\n",
       "                background-color: #6B9BD0;\n",
       "            }\n",
       "            .metrics-toggle {\n",
       "                cursor: pointer;\n",
       "                color: #3F7DC9;\n",
       "                text-decoration: underline;\n",
       "                font-size: 12px;\n",
       "                margin-top: 5px;\n",
       "                display: inline-block;\n",
       "                font-weight: bold;\n",
       "            }\n",
       "            .details-container {\n",
       "                display: none;\n",
       "                margin-top: 10px;\n",
       "                border-top: 1px solid #C4C7AC;\n",
       "                padding-top: 10px;\n",
       "            }\n",
       "    </style>\n",
       "    <script>\n",
       "    function toggleDetails(id) {\n",
       "        var details = document.getElementById('details-' + id);\n",
       "        if (details.style.display === 'none' || details.style.display === '') {\n",
       "            details.style.display = 'block';\n",
       "        } else {\n",
       "            details.style.display = 'none';\n",
       "        }\n",
       "    }\n",
       "    </script>\n",
       "    <div class=\"prompt-box\"><div style=\"background-color: #F0EBE5; font-size: 16px; font-weight: bold;\">Prompt</div>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt;&lt;/think&gt; and &lt;answer&gt;&lt;/answer&gt; tags, respectively, i.e., &lt;think&gt;reasoning process here&lt;/think&gt; &lt;answer&gt;answer here&lt;/answer&gt;. User: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?. Assistant: &lt;think&gt;</div><div class=\"response-container\"><div class=\"response-header\"><div>Response #1</div><div class=\"failure-badge\">FAIL</div></div><div class=\"response-body\">He earns <span class=\"math\">12/60</span>=$0.2/hour<br>So, he earns 0.2 * 50 = $&lt;&lt;0.<span class=\"math\">2*50</span>=10&gt;&gt;10 in an hour<br>So, he earns 10 * 2 = $&lt;&lt;<span class=\"math\">10*2</span>=20&gt;&gt;20<br>An hour.<br>So, he earns 20 * 60 = $&lt;&lt;<span class=\"math\">20*60</span>=1200&gt;&gt;1200 for babysitting for 6 hours.<br><span class=\"think-tag\">&lt;/think&gt;</span> <span class=\"answer-tag\">&lt;answer&gt;</span>1200<span class=\"answer-tag\">&lt;/answer&gt;</span></div><div class=\"metrics-container\"><div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score score-low\">14.8</span></div><div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score score-low\">-0.2</span></div><a class=\"metrics-toggle\" onclick=\"toggleDetails(0)\">Show component details</a><div id=\"details-0\" class=\"details-container\"><table class=\"component-table\"><tr><th>Component</th><th>Score</th><th>Weight</th><th>Contribution</th></tr><tr><td style=\"color: #2b8a3e; font-weight: bold\">format</td><td>10.0 / 10.0</td><td>1.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 100.0%; background-color: #2b8a3e;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">exact_answer</td><td>0.0 / 20.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">substring_answer</td><td>0.0 / 10.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #2b8a3e; font-weight: bold\">reasoning</td><td>8.0 / 10.0</td><td>0.6x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 80.0%; background-color: #2b8a3e;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">correct_final_answer</td><td>0.0 / 100.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr></table></div></div></div><div class=\"response-container\"><div class=\"response-header\"><div>Response #2</div><div class=\"failure-badge\">FAIL</div></div><div class=\"response-body\">It is $12/$hr x 50/(<span class=\"math\">60/60</span>) = <span class=\"calculation\">$&lt;&lt;<span class=\"math\">12/60*50</span>=6&gt;&gt;6/hour for one hour.<br>She watched <span class=\"math\">50/60</span> x 60 = 6.67 hour in total.<br>Therefore, She earned <span class=\"math\">6*6</span>.67 = &lt;&lt;<span class=\"math\">6*6</span>.67=40&gt;&gt;$</span>40 for babysitting.<br><span class=\"think-tag\">&lt;/think&gt;</span> <span class=\"answer-tag\">&lt;answer&gt;</span>40<span class=\"answer-tag\">&lt;/answer&gt;</span></div><div class=\"metrics-container\"><div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score score-low\">13.0</span></div><div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score score-low\">-0.3</span></div><a class=\"metrics-toggle\" onclick=\"toggleDetails(1)\">Show component details</a><div id=\"details-1\" class=\"details-container\"><table class=\"component-table\"><tr><th>Component</th><th>Score</th><th>Weight</th><th>Contribution</th></tr><tr><td style=\"color: #2b8a3e; font-weight: bold\">format</td><td>10.0 / 10.0</td><td>1.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 100.0%; background-color: #2b8a3e;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">exact_answer</td><td>0.0 / 20.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">substring_answer</td><td>0.0 / 10.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">reasoning</td><td>5.0 / 10.0</td><td>0.6x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 50.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">correct_final_answer</td><td>0.0 / 100.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr></table></div></div></div><div class=\"response-container\"><div class=\"response-header\"><div>Response #3</div><div class=\"failure-badge\">FAIL</div></div><div class=\"response-body\">For babysitting for 50 minutes or 0.5 hours, Weng earns $12 * 0.5 = $&lt;&lt;12*.5=6&gt;&gt;6.<br><span class=\"think-tag\">&lt;/think&gt;</span> <span class=\"answer-tag\">&lt;answer&gt;</span>6<span class=\"answer-tag\">&lt;/answer&gt;</span></div><div class=\"metrics-container\"><div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score score-low\">10.6</span></div><div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score score-low\">-0.4</span></div><a class=\"metrics-toggle\" onclick=\"toggleDetails(2)\">Show component details</a><div id=\"details-2\" class=\"details-container\"><table class=\"component-table\"><tr><th>Component</th><th>Score</th><th>Weight</th><th>Contribution</th></tr><tr><td style=\"color: #2b8a3e; font-weight: bold\">format</td><td>10.0 / 10.0</td><td>1.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 100.0%; background-color: #2b8a3e;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">exact_answer</td><td>0.0 / 20.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">substring_answer</td><td>0.0 / 10.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">reasoning</td><td>1.0 / 10.0</td><td>0.6x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 10.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">correct_final_answer</td><td>0.0 / 100.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr></table></div></div></div><div class=\"response-container\"><div class=\"response-header\"><div>Response #4</div><div class=\"failure-badge\">FAIL</div></div><div class=\"response-body\">12 * 50 /60 = &lt;&lt;<span class=\"math\">12*50/60</span>=10&gt;&gt;<span class=\"math\">10*60</span> = &lt;&lt;<span class=\"math\">10*60</span>=600&gt;&gt;600<br><span class=\"think-tag\">&lt;/think&gt;</span> <span class=\"answer-tag\">&lt;answer&gt;</span>600<span class=\"answer-tag\">&lt;/answer&gt;</span></div><div class=\"metrics-container\"><div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score score-low\">14.2</span></div><div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score score-low\">-0.2</span></div><a class=\"metrics-toggle\" onclick=\"toggleDetails(3)\">Show component details</a><div id=\"details-3\" class=\"details-container\"><table class=\"component-table\"><tr><th>Component</th><th>Score</th><th>Weight</th><th>Contribution</th></tr><tr><td style=\"color: #2b8a3e; font-weight: bold\">format</td><td>10.0 / 10.0</td><td>1.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 100.0%; background-color: #2b8a3e;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">exact_answer</td><td>0.0 / 20.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">substring_answer</td><td>0.0 / 10.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">reasoning</td><td>7.0 / 10.0</td><td>0.6x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 70.0%; background-color: #4dabf7;\"></div></div></td></tr><tr><td style=\"color: #e67700; font-weight: normal\">correct_final_answer</td><td>0.0 / 100.0</td><td>0.0x</td><td><div class=\"component-progress\"><div class=\"component-progress-bar\" style=\"width: 0.0%; background-color: #4dabf7;\"></div></div></td></tr></table></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(display_responses(\n",
    "    prompt=batch_input_ids, # NOTE: assume all prompts within batch are the same\n",
    "    responses=responses,\n",
    "    tokenizer=self._tokenizer,\n",
    "    grpo_size=self.grpo_samples,\n",
    "    advantages=advantages,\n",
    "    rewards=rewards,\n",
    "    successes=successes,\n",
    "    component_details=details,\n",
    "    show_n=4\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "del responses\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seq_lens = training.get_unmasked_sequence_lengths(response_padding_masks)\n",
    "\n",
    "### Mask out all the invalid values in the trajectory due to padding tokens\n",
    "logprobs[response_padding_masks] = 1.0\n",
    "ref_logprobs[response_padding_masks] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = [\n",
    "    GRPOTrajectory(\n",
    "        query_responses=query_responses,\n",
    "        logprobs=logprobs,\n",
    "        ref_logprobs=ref_logprobs,\n",
    "        rewards=rewards.reshape(batch_size * grpo_size),\n",
    "        successes=successes.reshape(batch_size * grpo_size),\n",
    "        advantages=advantages,\n",
    "        masks=masks,\n",
    "        position_ids=position_ids,\n",
    "        response_padding_masks=response_padding_masks,\n",
    "        seq_lens=seq_lens,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = GRPOTrajectory(*map(torch.cat, zip(*trajectories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_stats = []\n",
    "\n",
    "for _ in range(self._ppo_epochs):\n",
    "    # step_stats = self.grpo_step(trajectory, context_length)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unrolling `self.grpo_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.query_responses.shape # [grpo_size, n_tokens_generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.randint(0, trajectory.query_responses.shape[0]-1)\n",
    "pprint(self._tokenizer.decode(trajectory.query_responses[n].reshape(-1).tolist()))\n",
    "print('-'*80)\n",
    "pprint(self._tokenizer.decode(trajectory.query_responses[:, context_length:][n].reshape(-1).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pi_logits = self._model(\n",
    "    trajectory.query_responses,\n",
    "    input_pos=trajectory.position_ids,\n",
    "    mask=trajectory.masks,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_response_dim = pi_logits.shape[1] # pi_logits is tensor of shape [grpo_size, context_length + response_length, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_logits = rlhf.truncate_sequence_for_logprobs(pi_logits, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prior_response_dim - context_length == pi_logits.shape[1], 'rlhf.truncate_sequence_for_logprobs is not producing right context window.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_logprobs = rlhf.batched_logits_to_logprobs(\n",
    "    pi_logits,\n",
    "    trajectory.query_responses[:, context_length:],\n",
    "    self._temperature,\n",
    "    chunk_size=1,\n",
    ") # The log probabilities corresponding to each token in the generated sequence part of trajectory.query_responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pi_logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtune import rlhf\n",
    "from torchtune.rlhf import masked_sum\n",
    "\n",
    "\n",
    "class GRPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Group Relative Policy Optimization (GRPO) Loss module.\n",
    "    Introduced by https://arxiv.org/abs/2402.03300, popularized by https://arxiv.org/abs/2501.12948.\n",
    "\n",
    "    This loss implementation follows the usual formulation of GRPO with clipped ratios of token-wise logprobs.\n",
    "    Currently not validated to perform well.\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): clipping range for GRPO update.\n",
    "        kl_coeff (float): KL divergence coefficient (also known as beta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon: float = 0.1,\n",
    "        kl_coeff: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.kl_coeff = kl_coeff\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pi_old_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        pi_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        ref_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        advantages: torch.Tensor,  # [B x G]\n",
    "        padding_masks: Optional[torch.Tensor] = None,  # [B x G, L]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the GRPO loss module.\n",
    "\n",
    "        Args:\n",
    "            pi_old_logprobs (torch.Tensor): Log probabilities of the old policy. Shape: [batch_size * num_groups, seq_len]\n",
    "            pi_logprobs (torch.Tensor): Log probabilities of the current policy. Shape: [batch_size * num_groups, seq_len]\n",
    "            ref_logprobs (torch.Tensor): Log probabilities of the reference model. Shape: [batch_size * num_groups, seq_len]\n",
    "            advantages (torch.Tensor): Advantage values. Shape: [batch_size * num_groups]\n",
    "            padding_masks (Optional[torch.Tensor]): Padding token masks where True indicates tokens to include in loss calculation.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - loss: Total GRPO loss (policy loss + KL penalty)\n",
    "                - policy_loss: Clipped policy loss\n",
    "                - kl_loss: KL divergence loss between policy and reference model\n",
    "                - ratios: Mean ratio between current and old policy probabilities\n",
    "                - clipfrac: Fraction of clipped policy ratios\n",
    "        \"\"\"\n",
    "\n",
    "        ratios = torch.exp(pi_logprobs - pi_old_logprobs)  # [B x G, L]\n",
    "        clipped_ratios = torch.clamp(\n",
    "            ratios, 1.0 - self.epsilon, 1.0 + self.epsilon\n",
    "        )  # [B x G, L]\n",
    "\n",
    "        advantages = advantages[:, None]  # [B x G, 1]\n",
    "\n",
    "        policy_losses_clipped = advantages * clipped_ratios  # [B x G, L]\n",
    "        policy_losses_unclipped = advantages * ratios  # [B x G, L]\n",
    "\n",
    "        clipfrac = (\n",
    "            policy_losses_clipped < policy_losses_unclipped\n",
    "        ).float()  # [B x G, L]\n",
    "        clipfrac = rlhf.masked_mean(clipfrac, padding_masks)  # scalar\n",
    "\n",
    "        policy_loss = torch.minimum(\n",
    "            policy_losses_clipped, policy_losses_unclipped\n",
    "        )  # [B x G, L]\n",
    "        policy_loss = rlhf.masked_mean(policy_loss, padding_masks)\n",
    "\n",
    "        kl_loss = (\n",
    "            torch.exp(ref_logprobs - pi_logprobs) - (ref_logprobs - pi_logprobs) - 1\n",
    "        )  # [B x G]\n",
    "        kl_loss = rlhf.masked_mean(kl_loss, padding_masks)\n",
    "\n",
    "        loss = -(policy_loss - self.kl_coeff * kl_loss)\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            policy_loss.detach(),\n",
    "            kl_loss.detach(),\n",
    "            ratios.mean().detach(),\n",
    "            clipfrac.detach(),\n",
    "        )\n",
    "\n",
    "\n",
    "class GRPOCompletionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Group Relative Policy Optimization (GRPO) Loss module.\n",
    "    Introduced by https://arxiv.org/abs/2402.03300, popularized by https://arxiv.org/abs/2501.12948.\n",
    "\n",
    "    This loss implementation follows the usual formulation of GRPO with clipped ratios of full completion logprobs.\n",
    "    Currently not validated to perform well.\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): clipping range for GRPO update.\n",
    "        kl_coeff (float): KL divergence coefficient (also known as beta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon: float = 0.1,\n",
    "        kl_coeff: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.kl_coeff = kl_coeff\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pi_old_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        pi_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        ref_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        advantages: torch.Tensor,  # [B x G]\n",
    "        padding_masks: Optional[torch.Tensor] = None,  # [B x G, L]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the GRPO loss module.\n",
    "\n",
    "        Args:\n",
    "            pi_old_logprobs (torch.Tensor): Log probabilities of the old policy. Shape: [batch_size * num_groups, seq_len]\n",
    "            pi_logprobs (torch.Tensor): Log probabilities of the current policy. Shape: [batch_size * num_groups, seq_len]\n",
    "            ref_logprobs (torch.Tensor): Log probabilities of the reference model. Shape: [batch_size * num_groups, seq_len]\n",
    "            advantages (torch.Tensor): Advantage values. Shape: [batch_size * num_groups]\n",
    "            padding_masks (Optional[torch.Tensor]): Padding token masks where True indicates tokens to include in loss calculation.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - loss: Total GRPO loss (policy loss + KL penalty)\n",
    "                - policy_loss: Clipped policy loss\n",
    "                - kl_loss: KL divergence loss between policy and reference model\n",
    "                - ratios: Mean ratio between current and old policy probabilities\n",
    "                - clipfrac: Fraction of clipped policy ratios\n",
    "        \"\"\"\n",
    "\n",
    "        pi_old_logprobs = masked_sum(pi_old_logprobs, padding_masks)  # [B x G]\n",
    "        pi_logprobs = masked_sum(pi_logprobs, padding_masks)  # [B x G]\n",
    "        ref_logprobs = masked_sum(ref_logprobs, padding_masks)  # [B x G]\n",
    "\n",
    "        ratios = torch.exp(pi_logprobs - pi_old_logprobs)  # [B x G]\n",
    "        clipped_ratios = torch.clamp(\n",
    "            ratios, 1.0 - self.epsilon, 1.0 + self.epsilon\n",
    "        )  # [B x G]\n",
    "\n",
    "        policy_losses_clipped = advantages * clipped_ratios  # [B x G]\n",
    "        policy_losses_unclipped = advantages * ratios  # [B x G]\n",
    "\n",
    "        clipfrac = (policy_losses_clipped < policy_losses_unclipped).float()  # [B x G]\n",
    "        clipfrac = clipfrac.mean()  # scalar, only for logging\n",
    "\n",
    "        policy_loss = torch.minimum(\n",
    "            policy_losses_clipped, policy_losses_unclipped\n",
    "        )  # [B x G]\n",
    "        policy_loss = policy_loss.mean()  # scalar\n",
    "\n",
    "        kl_loss = (\n",
    "            torch.exp(ref_logprobs - pi_logprobs) - (ref_logprobs - pi_logprobs) - 1\n",
    "        )  # [B x G]\n",
    "        kl_loss = rlhf.masked_mean(kl_loss, padding_masks)\n",
    "\n",
    "        loss = -(policy_loss - self.kl_coeff * kl_loss)\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            policy_loss.detach(),\n",
    "            kl_loss.detach(),\n",
    "            ratios.mean().detach(),\n",
    "            clipfrac.detach(),\n",
    "        )\n",
    "\n",
    "\n",
    "class GRPOSimpleLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Group Relative Policy Optimization (GRPO) Loss module.\n",
    "    Introduced by https://arxiv.org/abs/2402.03300, popularized by https://arxiv.org/abs/2501.12948.\n",
    "\n",
    "    This loss implementation is based on TRL's implementation of GRPO,\n",
    "     which only takes a single gradient step per batch, trivializing some parts of the computation.\n",
    "     This empirically seems to perform well.\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): clipping range for GRPO update.\n",
    "        kl_coeff (float): KL divergence coefficient (also known as beta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon: float = 0.1,\n",
    "        kl_coeff: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.kl_coeff = kl_coeff\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pi_old_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        pi_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        ref_logprobs: torch.Tensor,  # [B x G, L]\n",
    "        advantages: torch.Tensor,  # [B x G]\n",
    "        padding_masks: Optional[torch.Tensor] = None,  # [B x G, L]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the GRPO loss module.\n",
    "\n",
    "        Args:\n",
    "            pi_old_logprobs (torch.Tensor): *UNUSED* Log probabilities of the old policy.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "            pi_logprobs (torch.Tensor): Log probabilities of the current policy.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "            ref_logprobs (torch.Tensor): *UNUSED* Log probabilities of the reference model.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "            advantages (torch.Tensor): Advantage values.\n",
    "                Shape: [batch_size * num_groups]\n",
    "            padding_masks (Optional[torch.Tensor]): Padding token masks where True indicates tokens to include in loss calculation.\n",
    "                Shape: [batch_size * num_groups, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - loss: Total GRPO loss (policy loss + KL penalty)\n",
    "                - policy_loss: Clipped policy loss\n",
    "                - kl_loss: KL divergence loss between policy and reference model\n",
    "                - ratios: Mean ratio between current and old policy probabilities\n",
    "                - clipfrac: Fraction of clipped policy ratios\n",
    "        \"\"\"\n",
    "\n",
    "        # [B x G, L]\n",
    "        per_token_kl = (\n",
    "            torch.exp(ref_logprobs.detach() - pi_logprobs)\n",
    "            - (ref_logprobs.detach() - pi_logprobs)\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        advantages = advantages[:, None]  # [B x G, 1]\n",
    "\n",
    "        per_token_policy_loss = (\n",
    "            torch.exp(pi_logprobs - pi_logprobs.detach()) * advantages\n",
    "        )\n",
    "\n",
    "        per_token_loss = -(per_token_policy_loss - self.kl_coeff * per_token_kl)\n",
    "\n",
    "        loss = rlhf.masked_mean(per_token_loss, padding_masks, dim=1).mean()\n",
    "\n",
    "        policy_loss = (\n",
    "            rlhf.masked_mean(per_token_policy_loss, padding_masks, dim=1)\n",
    "            .mean()\n",
    "            .detach()\n",
    "        )\n",
    "        kl_loss = rlhf.masked_mean(per_token_kl, padding_masks, dim=1).mean().detach()\n",
    "\n",
    "        return (  # This loss doesn't track clipfrac and ratios\n",
    "            loss,\n",
    "            policy_loss,\n",
    "            kl_loss,\n",
    "            torch.tensor(1.0),\n",
    "            torch.tensor(0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = GRPOSimpleLoss(kl_coeff = 0.01, epsilon = 0.2)\n",
    "loss, policy_loss, kl_loss, ratios, clipfrac = loss_fn(\n",
    "    trajectory.logprobs,\n",
    "    pi_logprobs,\n",
    "    trajectory.ref_logprobs,\n",
    "    trajectory.advantages,\n",
    "    padding_masks=~trajectory.response_padding_masks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
